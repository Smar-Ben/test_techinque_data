# Retail Data Pipeline - Architecture & Design

## üéØ Objectif

Construire un pipeline de donn√©es robuste pour ing√©rer, transformer et mettre √† disposition des donn√©es retail provenant d'une API REST dans un environnement analytique SQL compatible avec les outils de Business Intelligence.

## üìã Contexte

- **Source** : API REST retail (authentification par API key, format JSON)
- **Volume** : 1000 nouveaux enregistrements toutes les 10 minutes (~6000/heure)
- **Fr√©quence** : Rafra√Æchissement toutes les heures
- **Destination** : Base de donn√©es analytique requ√™table en SQL
- **Endpoints disponibles** : Sales, Products, Customers
- **Doublons**: On suppose que l‚ÄôAPI ne renvoie pas de doublons pour une m√™me p√©riode (hypoth√®se √† valider aupr√®s du fournisseur si possible)
- **Pr√©paration aux besoins futurs** : La table Customers, bien que non utilis√©e pour l‚Äôinstant, est int√©gr√©e d√®s maintenant afin d‚Äôanticiper de futurs besoins.

## üèóÔ∏è Architecture GCP

### Composants utilis√©s

| Composant            | R√¥le               | Justification                                                                                                                              |
| -------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |
| **BigQuery**         | Data Warehouse     | ‚Ä¢ Traitement de gros volumes ‚Ä¢ Requ√™table en SQL et optimis√© pour l'analytique ‚Ä¢ Serverless, pas de gestion d'instances                    |
| **Cloud Composer**   | Orchestration      | ‚Ä¢ Airflow manag√©, outil de r√©f√©rence pour l'orchestration ‚Ä¢ Personnalisation compl√®te des workflows                                        |
| **Cloud Run Jobs**   | Connecteur API     | ‚Ä¢ Cloud Run Jobs utilis√© pour des t√¢ches longues en mode batch (contrairement aux Cloud Functions qui traitent des t√¢ches √©v√©nementielles) |
| **Cloud Storage**    | Stockage fichiers  | ‚Ä¢ Solution optimale pour le stockage d'objets ‚Ä¢ Int√©gration native avec BigQuery ‚Ä¢ Archivage et audit des fichiers                         |
| **Cloud Monitoring** | Observabilit√©      | ‚Ä¢ Alerting sur les ressources Composer ‚Ä¢ Alerting et visualisation des co√ªts sur BigQuery                                                  |
| **Secret Manager**   | Gestion des secret | ‚Ä¢ Stockage s√©curis√© de l'API key ‚Ä¢ Rotation automatique des secrets ‚Ä¢ Int√©gration native avec Cloud Run                                    |

## ‚öôÔ∏è Cloud Run Jobs - Connecteurs API

### Fonctionnalit√©s

- **Authentification** : API key stock√©e dans Secret Manager (sur GCP)
- **Pagination automatique** : Gestion des limites API (max 250 items/page)
- **G√©n√©ration de fichiers uniques** : `{endpoint}_{YYYYMMDD_HHmm}_{uuid}.json`
- **Retry policy** : 3 tentatives avec exponential backoff
- **Idempotence** : Reprise granulaire par page avec checkpoint (√©vite le retraitement complet en cas d'√©chec)
- **Traitement asynchrone** : Pagination non-bloquante pour optimiser les performances
- **Initialisation dynamique** : R√©cup√©ration de la date de d√©part depuis BigQuery pour √©viter les doublons

### Arguments du Cloud Run Job

--endpoint : Endpoint √† traiter (sales/products/customers)
--start_sales_id : Date optionnelle au format YYYY-MM-DD (par d√©faut : date courante)

### Strat√©gies par endpoint

- **Sales** : Incr√©mental via `start_sales_id`
- **Products/Customers** : Full refresh (pas d'API incr√©mentale)

## üìä Structure des donn√©es

### Organisation des datasets BigQuery

On va organiser nos donn√©e avec une architecture en m√©daillon

```
üìÇ raw/                 # Donn√©es brutes de l'API
‚îú‚îÄ‚îÄ sales_api
‚îú‚îÄ‚îÄ products_api
‚îî‚îÄ‚îÄ customers_api

üìÇ ods/                 # Donn√©es nettoy√©es et exploitables
‚îú‚îÄ‚îÄ sales_clean
‚îú‚îÄ‚îÄ products_clean
‚îî‚îÄ‚îÄ customers_clean

üìÇ dmt/                 # Tables finales pour la BI (les data analysts auront acc√®s uniquement √† cette table)
‚îî‚îÄ‚îÄ sales_final
```

### Sch√©mas des tables principales

#### Dataset RAW

Contient trois tables (par types de fichier): sales, products et customers
Il faut partitionn√© ces tables par date d'ingestion (valeur SYS_DATE_CREATE) et stocker le nom du fichier (SYS_FILE_NAME)
Chaque table va √™tre aliment√© en append

```sql
-- Un enregistrement = une ligne de vente
CREATE TABLE raw.sales (
    id STRING,
    datetime STRING,
    total_amount STRING,
    customer_id STRING,
    items ARRAY<STRUCT<
        product_sku STRING,
        quantity STRING,
        amount STRING
    >>,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
) PARTITION BY DATE(SYS_DATE_CREATE);

-- Un enregistrement = un produit
CREATE TABLE raw.customers (
    customer_id STRING,
    emails ARRAY<STRING>,
    phone_numbers ARRAY<STRING>,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
) PARTITION BY DATE(SYS_DATE_CREATE);

-- Un enregistrement = un client
CREATE TABLE raw.products (
    product_sku STRING,
    description STRING,
    unit_amount STRING,
    supplier STRING,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
) PARTITION BY DATE(SYS_DATE_CREATE);
```

#### Dataset ODS

Contient trois tables (par types de fichier): sales, products et customers
On va avoir les donn√©es qui pourront √™tre exploit√© par notre dmt et on va transformer la donn√©e dans cette table.
Aussi il faut s'assurer de r√©cup√©rer uniquement les donn√©es non trait√©

```sql
-- Mode d'ingestion toujours en append, on ne devra pas stocker deux fois car control√©√© en amont via l'api
-- Si on a pas le contr√¥le alors on passe en merge avec
CREATE TABLE ods.sales (
    id INTEGER,
    datetime DATETIME,
    total_amount NUMERIC,
    customer_id STRING,
    items ARRAY<STRUCT<
        product_sku STRING,
        quantity INTEGER,
        amount NUMERIC
    >>,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
) PARTITION BY DATE(datetime);

-- Mode d'ingestion merge pour les customers, si on trouve une nouvelle information on va l'√©craser
-- Remarque: pour l'instant en merge j'√©crase mais on peut faire une √©volution pour g√©rer le changement (colonne SCD2)
CREATE TABLE ods.customers (
    customer_id STRING,
    emails ARRAY<STRING>,
    phone_numbers ARRAY<STRING>,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
);

-- Mode d'ingestion merge pour les products, si on trouve une nouvelle information on va l'√©craser
CREATE TABLE ods.products (
    product_sku STRING,
    description STRING,
    unit_amount NUMERIC,
    supplier STRING,
    SYS_DATE_CREATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    SYS_FILE_NAME STRING
);
```

#### Dataset dmt

Contient uniquement notre table finale qui va √™tre exploit√© par la BI

```sql
-- Mode d'ingestion toujours en append, on ne devra pas stocker deux fois car control√©√© en amont via l'api
-- Si on a pas le contr√¥le alors on passe en merge avec
CREATE TABLE dmt.sales_items (
    id STRING,--on veut avoir la cl√© sur une seule ligne donc concat entre sales.id et product.sku (s√©paration avec -)
    sales_datetime DATETIME,
    item_amount NUMERIC,
    product_sku STRING,
    item_quantity INTEGER,
    product_description STRING,
    discount_perc NUMERIC,
);

```

### Optimisations BigQuery

- **Ajout du clustering**: Voir selon nos besoin, mais le clustering peut √™tre utile pour am√©liorer la performance de nos requ√™tes
- **Ajout des description**: Ajouter les description sur nos tables pour avoir maximumn d'info
- **Ajout de contrainte**: On peut ajouter les primary key et foreign key sur nos tables pour am√©liorer les jointure

### Architecture des fichiers Cloud Storage

```
üìÇ gs://<bucket>/
sales/
‚îú‚îÄ‚îÄ in/                      # Fichiers JSON en attente de traitement
‚îú‚îÄ‚îÄ processing/              # Fichiers JSON en cours de traitement

‚îú‚îÄ‚îÄ archive/                 # Fichiers trait√©s avec succ√®s
‚îÇ   ‚îú‚îÄ‚îÄ 20250719/
‚îÇ   ‚îî‚îÄ‚îÄ 20250718/
‚îî‚îÄ‚îÄ failed/                  # Fichiers en erreur lors du traitement
‚îÇ   ‚îú‚îÄ‚îÄ 2024/01/15/10h/
‚îÇ   ‚îî‚îÄ‚îÄ ...
products/
‚îú‚îÄ‚îÄ in/
‚îú‚îÄ‚îÄ archive/
‚îî‚îÄ‚îÄ failed/
customers/
‚îú‚îÄ‚îÄ in/
‚îú‚îÄ‚îÄ archive/
‚îî‚îÄ‚îÄ failed/
```

## üîÑ Pipeline de donn√©es

### Vue d'ensemble du flux

```
API REST ‚Üí Cloud Run Jobs ‚Üí Cloud Storage ‚Üí BigQuery (raw) ‚Üí BigQuery (ods) ‚Üí BigQuery (dmt) ‚Üí Data Analysts
```

### Architecture des DAGs Airflow

#### 1. DAGs d'ingestion (x3)

- `dag_ingest_sales`: d√©clench√© toutes les heures (0 \* \* \* \*)
- `dag_ingest_products`: d√©clench√© une fois par jour (0 0 \* \* \*)
- `dag_ingest_customers`: d√©clench√© une fois par jour (0 0 \* \* \*)

**Remarque** : Il est n√©cessaire de rafra√Æchir enti√®rement les donn√©es products et customers √† chaque ex√©cution, car l'API ne fournit pas de m√©canisme d'incr√©mentation ou de filtrage (pas de updated_at, pas de pagination diff√©rentielle). Cela implique un dump complet √† chaque fois, ce qui peut devenir lourd en volume et co√ªteux en traitement √† mesure que les donn√©es croissent.

**Workflow par DAG :**

1. **Extract** : Appel API via Cloud Run Job avec gestion de la pagination et sauvegarde des fichiers JSON dans Cloud Storage
2. **File to RAW** : Insertion des donn√©es dans BigQuery (mode APPEND) dans le dataset raw (via un script dans airflow d√©placer les fichier en erreur)
3. **RAW to ODS** : Transformation de la donn√©e pour ins√©rer dans le dataset ods (zone silver) avec donn√©es nettoy√©es et exploitables
4. **Archive** : D√©placement des fichiers vers archive/ ou failed/ (en fonction du r√©sultat de la requ√™te)

Remarque: on peut ajouter une √©tape interm√©diaire pour stocker la date de la derni√®re insertion (si elle a r√©ussi ou non) pour exploiter cette information lors des prochaines ex√©cutions

#### 2. DAG de transformation finale

- `dag_transform_final`

**D√©clenchement :** Via Dataset (Asset) Airflow qui est d√©clench√© √† la fin du `dag_ingest_sales`

**Workflow :**

1. **Load Final** : Insertion des nouvelles donn√©es dans la table `sales_final` dans dmt

   Remarque : on suppose qu'on ins√®re pas les sales qui n'ont pas de produit ou customer r√©f√©renc√© dans la table finale

## ‚ö†Ô∏è Gestion des erreurs et qualit√© des donn√©es

### Qualit√© de donn√©es

Pour l'instant, il n'y a pas de gestion de qualit√© de donn√©es, mais plusieurs approches sont possibles :

**Gestion des orphelins** : le plus important pour les data analysts, ce sont les orphelins, on peut cr√©er une table dans le dag `dag_ingest_sales`

**M√©triques de qualit√© de donn√©es** : il est possible d'utiliser des outils plus pouss√©s comme **dbt** et/ou **Great Expectations** afin de valider la qualit√© de nos donn√©es directement dans nos dags

### Gestion des √©checs d'ingestion

**Processus :**

1. **√âchec d√©tect√©** ‚Üí D√©placement du fichier vers `failed/`
2. **Logging** ‚Üí Enregistrement de l'erreur au sein de la t√¢che airflow
3. **Notification** ‚Üí Alerte Airflow vers l'√©quipe Data Engineering (on peut sp√©cifier une liste de mails en cas d'√©chec)
4. **Investigation** ‚Üí Analyse du fichier brut pour debugging
5. **Rejeu** ‚Üí Possibilit√© de reprocesser depuis `failed/`

**√âvolution** : construire un dashboard de monitoring, pour chaque √©tape impliquant des tables BigQuery, on ajoute le r√©sultat dans une table airflow qui va √™tre exploit√©e par un dashboard (un exemple comme un autre), on peut observer la volum√©trie

## üîç Monitoring et observabilit√©

- **Volume** : Nombre d'enregistrements ing√©r√©s par endpoint
- **Qualit√©** : Taux d'orphelins, erreurs de format
- **Performance** : Temps d'ex√©cution des DAGs
- **Fiabilit√©** : Taux de succ√®s des ingestions
- **Alerting** : En cas d'√©chec d'ingestion ‚Üí Notification imm√©diate

## üèóÔ∏è Infrastructure & D√©ploiement

### Gestion de l'infrastructure

- **Infrastructure as Code** : Terraform pour provisionner et g√©rer les ressources GCP (Composer, Cloud Run, datasets BigQuery, IAM)
- **Environnements** : S√©paration dev/prod avec workspaces Terraform distincts (nombre d'environnements √† d√©finir selon les besoins)

## CI/CD Pipeline

- **Versioning**: Code stock√© sur Git avec workflow bas√© sur les Pull Requests
- **CI (Continuous Integration)** : Tests automatiques obligatoires avant merge
- **CD (Continuous Deployement)** : D√©ploiement automatique apr√®s validation PR (DAGs Airflow,images Docker Cloud Run)
- **Outils** : Pipelines natifs du repository (GitHub Actions / GitLab CI)
- **S√©paration des repos** : Projets distincts (Airflow, Cloud Run, Terraform) pour une meilleure gouvernance

### Strat√©gie de d√©ploiement

- **Validation** : Code review obligatoire + tests automatis√©s
- **D√©ploiement** : Manuel via approbation
- **Rollback** : Capacit√© de retour arri√®re rapide en cas de r√©gression

## S√©curit√© & Acc√®s

- **Service Accounts** : D√©di√©s par composant avec principe du moindre privil√®ge (service account pour composer, cloud run job, CD )
- **IAM** : Groupes IAM par profil m√©tier : data analyste, data engineer
- **Secrets** : API keys et credentials dans Secret Manager
- **Gestion des donn√©es personnelles**: La solution ne prend pas en compte la data gouvernance

## üöÄ √âvolutions & Am√©liorations

### Am√©lioration du pipeline de transformation

**dbt** (Data Build Tool) : Migration des transformations SQL vers dbt pour une meilleure gouvernance

- Tests de qualit√© de donn√©es int√©gr√©s
- Documentation automatique des mod√®les
- Lineage des donn√©es transparent
- Versioning des transformations

### Optimisation de l'ingestion

**API avec m√©tier** : Collaboration pour faire √©voluer l'API source

- Ajout de timestamps updated_at sur Products/Customers
- Endpoint de delta/changes pour √©viter les full refresh

### Monitoring avanc√©

- **Great Expectations** : Validation automatique de la qualit√© des donn√©es (possible de coupler avec dbt)
- **Dashboard de monitoring** : Centralisation des m√©triques via une table de logs BigQuery aliment√©e par chaque DAG (volum√©trie, temps d'ex√©cution, taux de succ√®s) et exploit√©e par un dashboard BI
